{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c95713",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b574fc14",
   "metadata": {},
   "source": [
    "NLTK (Natural Language Toolkit): NLTK is a widely-used library for NLP tasks such as tokenization, stemming, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85258cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27142aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc1597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "sentence = \"This is a sentence.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abbd10",
   "metadata": {},
   "source": [
    "# SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b9a73e",
   "metadata": {},
   "source": [
    "SpaCy is a library for advanced NLP tasks such as named entity recognition, part-of-speech tagging, and dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67703f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab472457",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c7f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify named entities in a sentence\n",
    "sentence = \"Netflix co-founder Reed Hastings to step down as chief executive\"\n",
    "doc = nlp(sentence)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3793e1",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70923046",
   "metadata": {},
   "source": [
    "Gensim is a library for topic modeling and document similarity analysis. (Bert Topic modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e578307",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a98cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# Preprocess and clean the text data\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "# Perform topic modeling on a collection of documents\n",
    "documents = [\"This is document 1.\", \"This is document 2.\", \"This is document 3.\"]\n",
    "processed_docs = [preprocess(doc) for doc in documents]\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary)\n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8fb0eb",
   "metadata": {},
   "source": [
    "# HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20cbdee",
   "metadata": {},
   "source": [
    "[Hugging Face Transformers](https://huggingface.co/docs/transformers/quicktour) is a library that provides pre-trained models and tools for natural language processing tasks such as language translation, text classification, and summarization, as well as for training custom models using the Transformer architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ce090",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a950792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae3a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\"We are very happy to attend the NLP lesson.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa2f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
